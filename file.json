{
    "articles": [
        {
            "layout": "post",
            "thumb": "oculus.jpg",
            "leadimg": "oculus.jpg",
            "tags": "Oculus Rift, VR, 3D",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "The Oculus Rift, VR and 3D",
            "description": "The history of virtual reality (VR) dates back to the 1950's. Since then, a lot of - sometimes quite exotic - devices have been developed.",
            "nerd": "2",
            "content": "The history of virtual reality (VR) dates back to the 1950's. Since then, a lot of - sometimes quite exotic - devices have been developed. For instance, take a look at this VR cabinet called \"sensorama\" developed by Morton Heilig in 1962:\n\n![Morton Heilig Sensorama](\/assets\/img\/blog\/sensorama.jpg)\n\nNowadays, most VR devices take the form of head mounted devices (HMD). Probably the best known example of such a device is the [Oculus Rift](https:\/\/www.oculus.com\/rift\/). The device looks a bit like safety goggles. Let's dive into some technical details of the Oculus Rift.\n\n![Oculus Rift and positional tracker](\/assets\/img\/blog\/oculus-rift-and-positional-tracker.jpg){: .with-caption}\n*The Oculus Rift Developer Kit 2 and positional tracker*\n\n### Displays and lenses\n\nFor each eye the Oculus has a full hd display on which the 3D content (for instance a game or a video) is rendered. The content has to be rendered in stereo which means that the image for the left display is taken from a slightly different angle compared to the image on the right display. This difference is analogous to the distance between our two eyes.\n\n![Early stereo image](\/assets\/img\/blog\/early-stereo-image.jpg){: .with-caption}\n*Example of an early stereo image*\n\n![Early stereo image, different camera positions](\/assets\/img\/blog\/early-stereo-image.gif){: .with-caption}\n*This shows this different camera positions of the photo*\n\nWe look at the image through a set of specially shaped lenses; these lenses distort the image in such a way that the field of view (FOV) becomes larger than the actual size of the displays in the Oculus. In the image below the letter X (in the red box) indicates the size of the real screen, the letter X' (X-prime) is the size of the screen you think you see because you look through the lenses:\n\n![Oculus lenses](\/assets\/img\/blog\/oculus-lenses.jpg)\n\nThe distortion of the image caused by the lenses is called pinch distortion and looks like this:\n\n![Pinch distortion](\/assets\/img\/blog\/pinch-distortion.jpg)\n\nTo cancel out the pinch distortion, the image is rendered with barrel distortion which looks like this:\n\n![Barrel distortion](\/assets\/img\/blog\/barrel-distortion.jpg)\n\nThe netto result of the pinch distortion of the lenses and the barrel distortion of the image is that you see a straight image that is bigger than the screen size of the Oculus.\n\nAs you can see in the image, a side effect of barrel distortion is that the image is stretched out towards the edges. This means that the pixel density is less in the outer regions of the image. This is not a problem, because it is much like how our own vision works in real life: the objects we see in our peripheral vision are not as sharp as the objects we see right in front of us. Shown in the image below: the red cone is the FOV that we can really focus on, and objects in the green and blue cones are increasingly more blurry.\n\n![FOV human eye](\/assets\/img\/blog\/FOV-human-eye.jpg)\n\n### Tracking rotation, movement and position\n\nThe Oculus has sensors that track rotation and the velocity of your movements; in the device you find a gyroscope, an accelerometer and a magnetometer.\n\nFurthermore, the Oculus has 40 leds that are tracked by the separate positional tracker device. This device looks a bit like a webcam and ideally you mount it on top of your computer monitor.\n\nThe data coming from all sensors and trackers gets combined in a process called [sensor fusion](http:\/\/en.wikipedia.org\/wiki\/Sensor_fusion). Sensor fusion roughly means that you combine data coming from different sources to calculate data that is more accurate than the data that comes from each individual source.\n\n\n### Generating the 3D scene\n\nThe Oculus has to be connected to a computer; an HDMI cable for the displays and a USB cable that attaches the connector box. The connector box receives both a cable from the positional tracker and from the HMD itself.\n\nOf all the data from the sensors are combined to create a 3D scene that is in accordance with the position and movement of your head and your body, which makes you feel like you are actually standing inside that scene.\n\nBecause the Oculus Rift blocks your vision on the real world and the fact that you are connected to a computer like a goat tied to a pole, it makes it quite hard - if not dangerous - to walk around while wearing an Oculus.\n\nTherefore, other devices have been developed that transfer physically walking movements to the computer as well, see images below. On the other hand, it is very likely that in the near future the on-board processor of the Oculus will be fast enough to render the 3D content and thus the Oculus Rift would become a standalone device, like Microsoft's Hololens.\n\n![VR treadmill](\/assets\/img\/blog\/VR-treadmill.jpg)\n\nThis device (currently on Kickstarter) takes it even further:\n\n<iframe height=\"360\" src=\"https:\/\/www.youtube.com\/embed\/bgblE3nxvNg\" frameborder=\"0\" allowfullscreen><\/iframe>\n\n### Other devices\n\nBesides the Oculus Rift there are numerous other companies that have made or announced HMD's for VR. You can roughly divide them into three categories: 1) devices that have to be connected to a computer, 2) devices that work with a mobile phone and 3) standalone devices.\n\nThe Oculus is of the first category; it needs a computer for rendering the content. On the one hand the HMD is an extra monitor to your computer, and on the other hand it is an input device that tracks your movements. In the future the connection between the HMD and the computer will probably become wireless.\n\nGoogles Cardboard is an example of the second category, the phone's gyroscope, accelerometer and magnetometer are used to track the rotation and position, and the 3D content is rendered by the phone itself.\n\nMicrosoft's Hololens is of the third category. With the increasing power of mobile processors and co-processors for rendering and motion, we will probably see more devices of this type in the future.\n\nAdvantage of the first category is that you have more processing power for rendering the 3D content, advantage of the second category is that you are not tied by wires to your computer and that it is a relatively cheap solution, provided that you already own a smartphone with decent processing power. The third category combines the advantages of the first two categories.\n\n\n<!--\nAt Tweede Golf we are currently researching how we can use VR together with our [3D framework](http:\/\/tweedegolf.nl\/3d-framework\/), and therefor we have acquired both an Oculus Rift and a Google Cardboard. In the coming blog posts we will share with you the results of this research.\n-->\n\n\nLinks:\n\n- [Barrell distortion](https:\/\/www.youtube.com\/watch?v=B7qrgrrHry0)\n- [Nvidia standalone HMD](http:\/\/vrfocus.com\/archives\/12225\/nvidia-vr-hmd-use-x1-super-chip\/)\n- [Oculus Rift teardown](https:\/\/www.ifixit.com\/Teardown\/Oculus+Rift+Development+Kit+2+Teardown\/27613)\n\n\n<!--\nMay be add a paragraph about latency and simulator sickness, see:\nhttps:\/\/docs.google.com\/a\/tweedegolf.com\/document\/d\/18ZT3Sea-Z9_Ve299WI9ui4bTmXv0Dk-cNHOjTBKzh8c\/edit#\n-->"
        },
        {
            "layout": "post",
            "thumb": "google-cardboard2.jpg",
            "leadimg": "google-cardboard2.jpg",
            "tags": "VR, WebVR",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "VR and WebVR",
            "description": "Virtual reality and the web. The state of WebVR.",
            "nerd": "2",
            "content": "Nowadays most VR applications are native games that are developed with tools like Unity and Unreal. These games have to be downloaded from the regular app stores, or from other app stores that have been set up by manufacturers of virtual reality headsets, like the [Samsungs Gear VR app store](https:\/\/www.oculus.com\/gear-vr\/).\n\nThe biggest benefit of native applications is their unbeatable performance, which is crucial for games. However, you can use VR for other purposes as well. For instance, you can add VR to [panorama viewers](http:\/\/www.emanueleferonato.com\/2014\/12\/10\/html5-webgl-360-degrees-panorama-viewer-with-three-js\/) to make them more immersive. Likewise, you could build 3D scenes that are architectural or historical recreations of buildings that you can enter and walk around in with your VR headset. These kind of applications are relatively easy to develop using web technologies.\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/127931214\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>{: .with-caption}\n*Panorama viewer by Emanuele Feronato*\n\nThe benefits of developing using open web technologies are obvious; you can publish your content instantly without gate keepers (app stores), you can use your own cheap or free tools, there is a culture of collaboration in the web developers' community, and so on. Both Mozilla and Google saw the potential of VR on the web and started to develop an API that provides access to VR devices. Currently only the Oculus Rift is supported, which will probably change as soon as new devices hit the market.\n\nMozilla and Google are working on one and the same API for WebVR, unlike what happened in the past with the development of the WebAudio API. Mozilla has also implemented WebVR in the nightly build of Opera. It is not yet known whether Spartan, Microsoft's new browser for Windows 10, is going to support WebVR. However, it probably is going to support WebVR, since so far Spartan has made a show of virtue as it comes to new browsers standards.\n\nGoogle also created an open source hardware VR device, the [Google Cardboard](https:\/\/www.google.com\/get\/cardboard\/). This is a device made of cardboard that turns a mobile device into a standalone VR headset. The mobile device's gyroscope, accelerometer and magnetometer are used to track the rotation and position, and the 3D content is rendered by the device itself. The Google Cardboard combined with the WebVR API and web technologies for generating the 3D scene makes creating VR application achievable for a large audience.\n\n![Google Cardboard](\/assets\/img\/blog\/google-cardboard2.jpg)\n\nThe WebVR API is able to detect a connected VR device, or if the browser is running on a device that can be used as a standalone VR device such as a mobile phone or a tablet. A single physical VR device shows up as a HMDVRDevice object and as a PositionSensorVRDevice object, but both objects share the same hardware id so you know they are linked. The first object contains information related to the display and the lenses such as the resolution, the distance between the lenses and the distance from your eyes to the lenses. The latter object contains information about the position, rotation and velocity of movement of the device.\n\nTo create the 3D content you can use a myriad of javascript 3D libraries, but Threejs is by far the most popular and easiest to use library. At Tweede Golf we continually check other libraries but so far we have stuck with Threejs. What's more, Threejs already supports VR devices; there are controllers that relay the tracking data from the sensors available, and renderers that do the stereo rendering for you.\n\nNow that [WebGL](http:\/\/caniuse.com\/#feat=webgl) has landed in all browsers across all operating systems, both mobile and desktop, the biggest hurdle for rendering 3D content in a browser is taken away. VR opens great opportunities to change the way we experience the web. For instance, Mozilla is experimenting to render existing web pages with CSS3 and WebGL for VR devices.\n\nIn the next blog post we show you our first test with WebVR.\n\nLinks:\n\n- [The Current Status of Browser-based Virtual Reality in HTML5](http:\/\/www.infoq.com\/news\/2015\/01\/vr-html5)\n- [A series of videos shot a the SFHTML5 meetup about VR and HTML5](https:\/\/www.youtube.com\/playlist?list=PLUj8-Hhrb-a0Z3f70ygX5fXLk8Sa4mTQZ)\n\n<!--\nThese are benefits that Tony Parisi mentioned in his speech for the SFHTML5 meetup that was held 16th of January this year:\n\n- instant access\n- no gatekeepers (app stores)\n- instant publishing\n- you can use your own (cheap or free) tools\n- culture of collaboration\n- source code (open)\n- no entry barriers\n-->"
        },
        {
            "layout": "post",
            "thumb": "osmose-wired.jpg",
            "leadimg": "osmose-wired.jpg",
            "tags": "VR, HMI, HCI",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "VR, HMI and HCI",
            "description": "A revolution in human-machine interaction? Virtual reality (VR) and augmented reality (AR) have received a revived interest due to the development of devices like the Oculus Rift and Microsoft's Hololens.",
            "nerd": "1",
            "content": "The interaction between a human and a computer, also called human-machine interaction (HMI) or human-computer interaction (HCI) has changed quite a lot in the past decades. Virtual reality (VR) and augmented reality (AR) have received a revived interest due to the development of devices like the Oculus Rift and Microsoft's Hololens. This considered, HCI will probably change even more radically in the coming years.\n\n### Short history\n\nHCI has been a topic of active research for decades; researchers and artists have invented the most exotic technologies, for instance, Char Davies' art project [Osmose](http:\/\/www.immersence.com\/osmose\/) whereby the user can navigate by breathing and moving her body.\n\n![Osmose suit](\/assets\/img\/blog\/osmose-suit.jpg)\n![Osmose suit wired](\/assets\/img\/blog\/osmose-wired.jpg){: .with-caption}\n*The vest is used to measure the breathing of the user*\n\nObviously, not every invention made it to the consumer market, but most technologies we use today have been invented long before they became mainstream. There are for instance striking similarities between Google Glass and [EyeTap](http:\/\/en.wikipedia.org\/wiki\/EyeTap) developed by Steve Mann in the 1980's and 1990's. <!--Wearable computing dates even further back, to the 17th century when the Chinese invented the [Abacus ring](http:\/\/gizmodo.com\/this-wearable-abacus-is-basically-the-worlds-oldest-sm-1545627562).-->\n\n![Eyetap vs Glass](\/assets\/img\/blog\/eyetap-vs-glass.jpg)\n![Eyetap development](\/assets\/img\/blog\/eyetap-development.jpg){: .with-caption}\n*Development of the EyeTap since 1980*\n\nWe have come a long way since the interaction with punched cards in the early days. In the 1960's the user interaction happened mostly via the command-line interface (CLI) and although the mouse was already invented in [1964](http:\/\/gajitz.com\/on-the-origin-of-mouse-first-mouse-nearly-lost-to-history\/), it became only mainstream with the advent of the graphical user interface (GUI) in the early 1980's. GUI's also made it more apparent that HCI is actually a two-way communication; the computer receives its input via the GUI and also gives back the output or the feedback via the GUI.\n\n![First mouse](\/assets\/img\/blog\/first-mouse.jpg){: .with-caption}\n*First mouse as invented by Douglas Engelbart*\n\n### NUI and gestures\n\nSpeech control became consumer-ready in the 1990's (though very expensive back then). Interesting about speech control is that it is the first appearance of a Natural User Interaction (NUI). NUI roughly means that the interface is so natural that the user hardly notices it. Another example of NUI is touchscreen interaction, though we have to distinguish between using touch events as replacement for mouse clicks, such as tapping on button element in the GUI, and gestures, for instance a pinch gesture to scale a picture. The latter is NUI, the former is a touch-controlled GUI.\n\nInstead of making gestures on a touch screen, you can also perform them in the air in front of a camera or a controller such as the [Leap Motion](https:\/\/www.leapmotion.com\/). Gestures can also be made while wearing a [data glove](https:\/\/www.vrealities.com\/products\/data-gloves)\n\n![Data glove](\/assets\/img\/blog\/data-glove.jpg)\n\n### Interaction with brainwaves\n\nWearables such as smart watches are usually a mix between a remote controller and an extra monitor of a mobile device. As a remote controller you can send instructions like on a regular touchscreen, but for instance the Apple Watch has a classic rotary button for interaction as well. Wearables can also communicate other types of data coming passively from a human to the computer, like heart rate, skin temperature, blood oxygen and probably a lot more to come when more types of sensors become smaller and cheaper.\n\nGoogle Glass is a wearable that can be controlled by voice and by brainwaves. By using a telekinetic headband that has sensors for different areas of the brain, brainwaves are turned from passive data into an actuator. Fields of application are typically medical aids for people with a handicap.\n\n![Google Glass with telekinetic headband](\/assets\/img\/blog\/google-glass-with-telekinetic-headband.jpg){: .with-caption}\n*Showing a headband with 3 sensors on the skull and one that clips onto the user's ear*\n\n### AR and VR\n\nWith AR a digital overlay is superimposed on the real world whereas with VR the real world is completely replaced by a virtual (3D) world. Google Glass and Hololens are examples of AR devices. The Oculus Rift and Google Cardboard are examples of VR devices.\n\nGoogle Glass renders a small display in front of your right eye and the position of this display in relation to your eye doesn't change if you move your head. Hololens on the other hand actually 'reads' the objects in the real world and is able to render digital layers on top of these objects. If you move your head, you'll see both the real world object and the rendered layer from a different angle.\n\n![Hololens rendering interfaces on real world objects](\/assets\/img\/blog\/hololens-rendering.jpg){: .with-caption}\n*Hololens rendering interfaces on real world objects*\n\nAR is very suitable for creating a Reality User Interface (RUI), also called a Reality Based Interface (RBI). In a RBI real world objects become actuators; for instance, a light switch becomes a button that can be triggered with a certain gesture. An older and more familiar example of RBI is when a 3D scene is rendered on top of a marker; when you rotate the marker in the real world, the 3D scene will rotate accordingly. Instead of a marker you can also use other real world entities, for instance, Layar makes use of the GPS data of a mobile device.\n\nVR is commonly used for immersive experiences such as games, but it can also be used to experience historical or future scenes like building that have been designed but haven't been built yet.\n\n![AR Basketball App Mug](\/assets\/img\/blog\/AR-Basketball-App-Mug.jpg){: .with-caption}\n*An example of a RBI: a marker is used to control a 3D scene*\n\n### Researching VR for web\n\nWe will be looking at two VR devices in the near future: the Oculus Rift and Google Cardboard. In the coming blog posts we will share the results with you.\n\nLinks:\n\n- [NUI](http:\/\/en.wikipedia.org\/wiki\/Natural_user_interface)\n- [Wearables](http:\/\/en.wikipedia.org\/wiki\/Wearable_computer)\n- [Osmose](http:\/\/www.immersence.com\/osmose\/)\n- [Multitouch](http:\/\/www.ted.com\/talks\/jeff_han_demos_his_breakthrough_touchscreen?language=en#t-10169) The video is made in 2006: note how enthusiastic the audience is about multi touch control, nowadays multi touch control is part of our daily life.\n- [Brainwaves](http:\/\/www.digitaltrends.com\/mobile\/mindrdr-controlling-google-glass-with-your-mind\/)\n- [First mouse](http:\/\/gajitz.com\/on-the-origin-of-mouse-first-mouse-nearly-lost-to-history\/)\n- [Hololens](http:\/\/www.microsoft.com\/microsoft-hololens\/en-us)\n\n<!--\nA mouse click on a button in a GUI can be seen as a shortcut for a command.\n\ncovers the two-way communication between a human and a machine. Usually the human gives the machine an instruction and the machine provides the human with feedback as soon as the instruction has been processed.\n-->"
        },
        {
            "layout": "post",
            "thumb": "threejs-barrel-distortion.jpg",
            "leadimg": "threejs-barrel-distortion.jpg",
            "tags": "VR, WebVR, 3D, Cardboard, Three.js",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "VR, WebVR and Cardboard",
            "description": "Our first WebVR application is a big cube in Threejs and a simple 3D scene floating inside that cube. The 3D scene consists of ...",
            "nerd": "3",
            "content": "Let's start simple. Our first WebVR application is a big cube in Threejs and a simple 3D scene floating inside that cube. The 3D scene consists of a transparent floor with a few simple rectangular shapes placed on it.\n\nOn each side of the cube we print the name and direction of the axis towards which the side is facing. Lets call this cube the \"orientation cube\", and lets call the 3D scene \"the world\" because that is what it is from the user's perspective. Both the orientation cube and the world are directly added to the root scene, which is the scene you create with the code `rootScene = new THREE.Scene()`.\n\nWhen wearing an Oculus, you are positioned in the middle of this orientation cube and you can look to all sides of the cube by moving your head. You can move around in the world with the arrow keys of your keyboard.\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/127927605\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>\n\n<!--\nA real world analogy would be when a camera is mounted on a small cart that you can drive around in a miniature world, and this miniature world is placed inside a room that has the axis directions printed in large letters on all four walls, on the floor and on the ceiling.\n-->\n\n### The API\n\nTo get the rotation and position data of the Oculus using javascript, we first query for VR devices:\n\n~~~javascript\nif(navigator.getVRDevices){\n  \/\/ getVRDevices returns a promise\n  navigator.getVRDevices().then(\n    \/\/ on fulfilled callback returns an array containing all detected VR devices\n    function onFulFilled(data){\n      detectedVRDevices = data;\n      onFulfilled(deviceData);\n    }\n  );\n}\n~~~\n\nThe detected VR devices can be instances of `PositionSensorVRDevice` or instances of `HMDVRDevice`.\n\n`PositionSensorVRDevice` instances are objects that contain data about rotation, position and velocity of movement of the headset.\n\n`HMDVRDevice` instances are objects that contain information such as the distance between the lenses, the distance between the lenses and the displays, the resolution of the displays and so on. This information is needed for the browser to render the scene in stereo with barrel distortion, like so:\n\n![ThreeJS barrel distortion](\/assets\/img\/blog\/threejs-barrel-distortion.jpg)\n\nTo get the rotation and position data from the `PositionSensorVRDevice` we need to call its `getState()` method as frequently as we want to update the scene.\n\n~~~javascript\nfunction vrRenderLoop(){\n  let state = vrInput.getState();\n  let orientation = state.orientation;\n  let position = state.position;\n\n  if(orientation !== null){\n    \/\/ do something with the orientation,\n    \/\/ for instance rotate the camera accordingly\n  }\n\n  if(position !== null){\n    \/\/ do something with the position,\n    \/\/ for instance adjust the distance between the camera and the scene\n  }\n\n  \/\/ render the scene\n  render();\n\n  \/\/ get the new state as soon as possible\n  requestAnimationFrame(vrRenderLoop);\n}\n~~~\n\n### Putting it together\n\nFor our first application we only use the orientation data of the Oculus. We use this data to set the rotation of the camera which is rather straightforward:\n\n~~~\n  let state = vrInput.getState();\n  camera.quaternion.copy(state.orientation);\n~~~\n\nUsually when you want to walk around in a 3D world as a First Person you move and rotate the camera in the desired direction, but in this case this is not possible because the camera's rotation is controlled by the Oculus. Instead we do the reverse; keeping the camera at a fixed position while moving and rotating the world.\n\nTo get this to work properly, we add an extra pivot to our root scene and we add the world as a child to the pivot:\n\n~~~\ncamera\nroot scene\n  \u21b3 orientation cube\n  \u21b3 pivot\n        \u21b3 world\n~~~\n\n<!--\nA real world analogy would be a room (root scene) that has the axis directions painted in large letters on all four walls, the floor and the ceiling (orientation cube). In the room we put a table (the pivot) and on this table a miniature world (the 3D scene) is placed.\n\nThe pivot is necessary because we want the current position in the scene to be the rotation point (the pivot). You can visualize how this works by putting a pencil upright on your desk and hold a piece of paper (or any other flat object) above the tip of the pencil. The desk is the root scene, the pencil is the pivot and the piece of paper is the 3D scene.\n\nNow if we want to rotate, we rotate the piece of paper around the point where the tip of the pencil touches the paper (the pivot point), and if we want to change our position we move the piece of paper over the pencil's tip in the desired direction (thus we change the position of the pivot point). See this :\n-->\n\nThe camera (the user) stays fixed at the same position as the pivot, but it can rotate independently of the pivot. This happens if you rotate your head while wearing the Oculus.\n\nIf we want to rotate we world, we rotate the pivot. If we want to move forward in the world, we move the world backwards over the pivot, see this video:\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/127927690\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>\n\nYou can try it yourself with the [live version](http:\/\/data.tweedegolf.nl\/vr-test4\/); the arrow keys up and down control the translation of the world and the arrow keys left and right the rotation of the pivot. The source code is available at [GitHub](https:\/\/github.com\/tweedegolf\/vr-test4).\n\n<!--\n**Threejs specific topics**\n\nIn Threejs a non-rotated 3D object faces towards the positive z-axis and the top of a 3D object is in the direction of the positive y-axis. In the image below the red line is the x-axis, the green line the y-axis and the blue line the z-axis. A dotted line indicates a negative axis.\n\n<img src=\"\/assets\/img\/blog\/threejs-axis.jpg\" width=\"65%\">\n\nAccording to Threejs, the arrow in the picture above has a rotation of 0\u00b0 on the z-axis (and on the other 2 axes as well for that matter). However in trigonometry a 0\u00b0 rotation over the z-axis is a vector in the direction of the positive x-axis, so the real rotation of the arrow in the picture is 90\u00b0.\n\nIf we want to move the arrow one unit in the direction towards which the arrow is rotated, we use trigonometry to calculate the fraction of the unit the arrow has to move over the x-axis and over the y-axis, therefor we have to compensate for Threejs' unorthodox reading of a rotation of 0\u00b0:\n\n~~~\n  arrow.position.x += unit * Math.cos(arrow.rotation.z + Math.PI\/2);\n  arrow.position.y += unit * Math.sin(arrow.rotation.z + Math.PI\/2);\n~~~\n\nAnother thing that we can learn from the image above is that in order to make a ground for our 3D scene, we need to rotate a the arrow by -90\u00b0 over the x-axis:\n\n<video height=\"360\" controls>\n  <source src=\"http:\/\/data.tweedegolf.nl\/videos\/plane_rotation.mp4#t=0.07\" type=\"video\/mp4\" loop>\n<\/video>\n\nInstead of rotating the arrow to make a floor, you could also choose to rotate the whole root scene. And you could perform a rotation over the z-axis at the same time to compensate for the fact that 0\u00b0 in Threejs is actually a rotation of 90\u00b0:\n~~~\n  scene.rotation.x -= Math.PI\/2\n  scene.rotation.z += Math.PI\/2\n~~~\n\nIf you choose to rotate the scene, please make sure that you do not add the camera to the scene, see next section.\n-->\n\n### About the camera in Threejs\n\nThe camera in Threejs is on the same hierarchical level as the root scene by default. Which is like a cameraman who is filming a play on a stage while standing in the audience; theoretically both the stage and the cameraman can move, independently of each other.\n\nIf you add the camera to the root scene then it is like the cameraman stands on the stage while filming the play; if you move the stage, the cameraman will move as well.\n\nYou can also add a camera to any 3D object inside the root scene. This is like the cameraman standing on a cart on the stage while filming the play; the cameraman can move independently of the stage, but if the stage moves the cameraman and her cart will move as well.\n\nIn our application the camera is fully controlled by the Oculus, so the first scenario is the best option.\n\nThis is comes in handy, since we have applied rotations to the root scene (see in [this post](\/2015\/04\/17\/threejs-rotations\/)). As a consequence, if we add the camera to the scene, the rotations of the scene will have no effect. Here is an example of a situation whereby the scene rotates while the camera is added to that same scene:\n\n<iframe height=\"360\" src=\"https:\/\/www.youtube.com\/embed\/R2Ch397Ipps\" frameborder=\"0\" allowfullscreen><\/iframe>\n\nNote that in most Threejs examples you find online it does not make any difference whether or not the camera is added to the root scene, but in our case it is very important.\n\n\n### The result\n\nWe have made two screencasts of the result from the output rendered to the Oculus:\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/127927801\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/127927800\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>\n\n<!--\nIf the application runs on a mobile device inside a browser that doesn't have the WebVR API implemented, we use Threejs' `DeviceOrientationControls` to get the rotation of the device.\n-->\n\n<!--\nSince early April, mobile devices are being detected as VR devices. This means that you can use the WebVR API for Cardboard applications as well; you don't need the native Cardboard API anymore.\n\nTo distinguish between mobile devices and the Oculus Rift (or other HMD's), we perform a check to determine on what operating system our application is running.\n\nIf no VR device is detected and the application is not running on a mobile device, the scene is rendered by the default WebGL renderer of Threejs. Otherwise we render the scene using the `StereoEffect` that comes with Threejs: this effect renders the scene in stereo.\n\n<img src=\"\/assets\/img\/blog\/threejs-barrel-distortion.jpg\" width=\"65%\">\n-->"
        },
        {
            "layout": "post",
            "thumb": "floor-and-arrow.png",
            "leadimg": "floor-and-arrow.png",
            "tags": "Threejs, first person, WebGL",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "Threejs rotations",
            "description": "Using rotations in Threejs to create a a first person 3D setting.",
            "github": "https",
            "nerd": "3",
            "content": "In this post we create a first person 3D setting, and we use rotations to accomplish this.\n\nIn Threejs you create a scene like so:\n\n~~~\n let scene = new THREE.Scene();\n~~~\n\nThis is the root scene; all other 3D objects have to be added to this root scene to make them visible:\n\n~~~\n  let obj3D = new THREE.Object3D(); \/\/ create a 3D object\n  scene.add(obj3D);\n~~~\n\nThe coordinate system of Threejs is a right handed system, which means that the positive z-axis is pointing towards you:\n\n![Left and right handed system](\/assets\/img\/blog\/left-and-right-handed-system.gif)\n\nIn the picture below you see a Threejs scene. The red line is the x-axis, the green line the y-axis and the blue line the z-axis. A dotted line indicates a negative axis.\n\nThe black arrow in the yellow square is an instance of `THREE.PlaneBufferGeometry` which is a subclass of `THREE.Object3D`, the basic 3D object in Threejs.\n\nOn this PlaneBufferGeometry a texture of an arrow has been mapped. The PlaneBufferGeometry has been added to the root scene without any rotation or translation.\n\n![ThreeJS axis](\/assets\/img\/blog\/threejs-axis.jpg)\n\nWhat we can learn from this picture, is that a 3D object without translation and rotation gets added to the origin of the scene.\n\nAnd as far as rotation is concerned: a 3D object that has no rotation on any of the three axes stands perpendicular to our line of sight and has its upside in the direction of the positive y-axis.\n\n### Creating a floor\n\nIf we want to create a floor, or a ground for our 3D scene we have to rotate a plane -90\u00b0 over the x-axis, play the following video to see how that works out:\n\n<video height=\"360\" controls>\n  <source src=\"http:\/\/data.tweedegolf.nl\/videos\/plane_rotation.mp4#t=0.07\" type=\"video\/mp4\">\n<\/video>\n\nIf you rotate a 3D object in Threejs you only change its rotation in relation to the root scene: its own coordinate system is not affected. In the video above the positive y-axis of the PlaneBufferGeometry gets aligned with negative z-axis of the root scene.\n\nWhat we could do as well, is to apply the rotation to the root scene as a whole; in that case the axes of the floor and the root scene stay aligned with each other:\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/127927799\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>\n\nBoth solutions are equally valid, but there is one caveat: if you choose to rotate the root scene, please make sure that you do *not* add the camera to the scene because that would cancel out the rotations, see this [post](\/2015\/04\/03\/webvr-and-threejs\/#camera).\n\n### Moving over a floor\n\nNow lets create a proper floor and add the arrow object to the floor:\n\n![Floor and arrow](\/assets\/img\/blog\/floor-and-arrow.png)\n\nNext we want to move the arrow object one unit into the direction the arrow head is pointing. We use trigonometry to calculate the fraction of the unit the arrow object has to move over the x-axis and the fraction of the unit the arrow object has to move over the y-axis based on its rotation over the z-axis:\n\n~~~\n  arrow.position.x += unit * Math.cos(arrow.rotation.z);\n  arrow.position.y += unit * Math.sin(arrow.rotation.z);\n~~~\n\nBecause the z-rotation of the arrow object is 0\u00b0 this boils down to:\n\n~~~\n  Math.cos(0) = 1;\n  Math.sin(0) = 0;\n\n  arrow.position.x += unit;\n  arrow.position.y += 0;\n~~~\n\nThe translation of the arrow object on the x-axis is 1 unit, and the translation on the y-axis is 0, which means that the arrow object is moving over the red line (the x-axis) to the right instead of over the green line (the y-axis) away from us.\n\nThis is rather counter intuitive, and it is the result of the fact that in Threejs the top of a 0\u00b0 rotated 3D object is in the direction of the positive y-axis, which is a very understandable decision because the y-axis is usually the vertical\/upright axis.\n\nWe can fix this by rotating the floor or the root scene 90\u00b0 over the z-axis. Lets move the root scene so the axes of the floor stay aligned with the axes of the root scene:\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/127927743\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>\n\nThe arrow object is moving away from us but the head of the arrow points a the wrong direction. The rotation of the arrow object is still is 0\u00b0, but the texture on the arrow object (the PlaneBufferGeometry instance) makes us believe that the arrow object has a rotation of -90\u00b0.\n\nWe fix this in the arrow object itself by rotating the texture 90\u00b0 which makes the direction of the arrow head consistent with the rotation of the PlaneBufferGeometry that it is applied to.\n\n<!--\nYou might think this is a dirty trick, but it isn't; Threejs has chosen to make the top of a 0\u00b0 rotated object in the direction of the positive y-axis which is a very understandable decision because the y-axis is usually the vertical\/upright axis.\n-->\n\n### Conclusion\n\nIf we rotate the root scene (or the floor) -90\u00b0 over the x-axis, the y-axis becomes the 'away into the distance' axis, the natural axis that we want to move along when moving straight forward.\n\nBut because the natural angle of a straight forward movement is 0\u00b0, the x-axis actually is the most natural axis for moving forward, so we rotate the root scene (or the floor) 90\u00b0 over the z-axis as well to swap the x and the y-axis.\n\nNow we have created the ideal situation for a first person setting.\n\nYou can play yourself with the [final result](http:\/\/data.tweedegolf.nl\/threejs-rotations). Code is available at [GitHub](https:\/\/github.com\/tweedegolf\/threejs-rotations).\n\n<!--\nAnother example: we want to translate the arrow after we have rotated the arrow 45\u00b0. A positive rotation in Threejs follows the convention and is counterclockwise:\n\n![ThreeJS rotation 45 degrees](threejs-rotation-45-degrees.jpg)\n\nNow we get this:\n\n~~~javascript\n  \/\/ Math.PI\/4 = 45\u00b0, Math.PI\/2 = 90\u00b0\n  Math.cos(Math.PI\/4 + Math.PI\/2) = -0.707;\n  Math.sin(Math.PI\/4 + Math.PI\/2) = 0.707;\n~~~\n\nThis results in an equal translation over both the x and the y-axis, and the translation over the x-axis is in the negative direction. This is exactly what we want.\n-->"
        },
        {
            "layout": "post",
            "thumb": "skauti.jpg",
            "leadimg": "skauti.png",
            "tags": "Threejs, WebGL, 3D, Data visualization",
            "description": "3D data visualization for geospatial analysis using WebGL and Three.js",
            "title": "3D data visualization",
            "author": "RubenN",
            "contact": "RubenN",
            "about": "3D data visualization",
            "nerd": "2",
            "content": "At tweede golf, we value innovation: we take the time to research new technologies and subsequently challenge ourselves to try out these new techniques in order to discover new applications. We also like to learn by doing: build something first, ask questions later. \n\nFollowing that philosophy, we recently held a programming contest. We gave ourselves two days to create new applications on top of our existing [3D framework]. One of the teams created an app they dubbed \"Skauti\". It uses a 3D representation to visualize datasets. \n\nNow is the time to look back: what are the benefits of 3D data visualization? Read on and find out. \n\n![The Skauti prototype](\/assets\/img\/blog\/skauti.png){:.with-caption}\n*The Skauti prototype*\n\nWe all like to base our decisions on data. However, just having a data sheet containing a lot of numbers will often not help you much, especially when the data set is very large. Graphs and visualizations are typcially used to obtain a better understanding of data. When it comes to presenting geospatial data (i.e. data which is dependent on some location) a 2D map is often the preferred solution.\n\n### Making sense of data\n\nFor centuries, cartographers (map-makers) have used projections and symbolism\nto create a 2D interpretation of the actual world. Nowadays, 2D maps are still widely used, as are traditional tools like adding colour, using markers and higlighting areas to indicate special points of interest with the goal of providing as much insight into the data as possible. \n\n![City size in Devon, England](\/assets\/img\/blog\/circle_map.png){:.with-caption}\n\n###  2D vs 3D\n\nA more detailed look at the map above (which is a visualization of city sizes in Devon, England) reveals the limitations of 2D visualizations.\nDid you notice the tiny 35 (Topsham) under Exeter? And which city is bigger: Torquay or\nPaignton? Their bubbles overlap, making it harder to identify their sizes.\n\nHow to solve these problems? If we could use the third dimension as well, we could use a height\ninstead of the circle radius to indicate city sizes. In addtion, we can simply choose a\nradius which makes sure there are no overlaps, making it easier to interpret\nthe data.\n\nWhat about the use of colour? Sometimes color is used to indicate\nsome value (for example in a heatmap). But what does green signify and how should we interpret red? If we compare the two maps of Mount Taranaki in New Zealand we can find below (one a traditional 2D height map, one a 3D representation), it is immediately apparent that the 3D version gives us more detailed information and it presents us with a more intuitive understanding of the mountain.\n\n![Mt. Taranaki, New Zealand](\/assets\/img\/blog\/mt-taranaki-colors.png){:.with-caption}\n*&copy; CC BY-SA [Koordinates](https:\/\/koordinates.com)*\n\n![Mt. Taranaki, New Zealand](\/assets\/img\/blog\/mt-taranaki-3d.png){:.with-caption}\n*&copy; CC BY-SA*\n\n3D visualizations can be run in your webbrowser using WebGL with vector based approaches\ninstead of the pixel based tile maps often used for 2D cartography on the web. Not only does this look great, it has some more advantages: interaction\nbecomes easier to achieve and scrolling and zooming can be made into a more\nsmooth experience for the user.\n\n### The Skauti prototype \n\nWe wanted to make the most of these advantages. We set ourselves the challenge to create a small prototype of a 3D map. In this prototype we took our own city, Nijmegen, and we used building data provided by the Dutch government, specifically\nthe [BAG]{:target=\"_blank\"} and [AHN2]{:target=\"_blank\"} datasets, to determine\nwhere buildings are and how tall they are. We used these datasets before to create fancy [point cloud visualizations].\n\nWe then picked some houses in\nour neighbourhood from Funda (a Dutch website listing properties for sale and for\nrent) and assigned them a color based on whether they were for sale\nor for rent. We also built a quick animation that gives the user access to more detailed information at the top of the screen.\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/133463503\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>{: .with-caption}\n*3D data visualization (dubbed \"Skauti\")*\n\nAbove you can see what typical user interaction in this\nprototype is like. Given the fact it only took us the time span of a two-day programming contest to make this prototype, we can only imagine what can be achieved using this technique. If you see an application that is useful to you, do not hesitate to contact us.\n\n[3D framework]: \/3d-framework\/\n[point cloud visualizations]: \/#portfolio-planviewer-3d\n[BAG]: https:\/\/data.overheid.nl\/data\/dataset\/basisregistratie-adressen-en-gebouwen-bag-\n[AHN2]: https:\/\/data.overheid.nl\/data\/dataset\/ahn2-0-5-meter-ruw-raster"
        },
        {
            "layout": "post",
            "title": "\"Point Light Shadows In Three.js, part II\"",
            "date": "\"2015-08-02\"",
            "description": "Follow up post on point light shadows in Three.js.",
            "thumb": "light2-shader.png",
            "leadimg": "light2-shader.png",
            "tags": "Three.js, point light, shadow casting, WebGL",
            "author": "Dennis",
            "contact": "RubenN",
            "about": "Point light shadows in Three.js",
            "nerd": "5",
            "content": "While working on [a 3D project] that involved garden lights we stumbled upon unexpected problems with shadows cast from point lights. The shadows were either not there at all or they were just all over the place. It seemed impossible to create a decent light\/shadow world (garden in our case). After recovering from the initial shock and disappointment we started investigating the problem. This should be doable. As it turns out, it sort of is. Read on.\n\nIn the [previous blog post] about this topic we talked about possible ways to implement an efficient way of calculating point light shadows in [Three.js], the javascript 3D library. 'Efficient' meaning: taking fewer texture samplers than the naive approach which takes six samplers for each point light, one for each of the principal directions.\n\nWe came up with three potential solutions:\n\n* Divide a larger texture into smaller viewports and draw the depth map to each of these viewports\n* Render each of the depth maps to one side of a cube texture map\n* Use dual-hyperboloid shadow mapping\n\nWe will first discuss the three approaches and finish up with the results of the most successful one.\n\n### Possible solutions\n\nThe first approach - dividing a larger texture into smaller viewports - proved to be difficult to integrate with the current shader and uniform variables.\nThe GLSL shader language requires all of the for loops to be unrollable, as the shaders can then be divided into small chunks of work for each of the stream processors of the GPU.\nWhen handling an assortment of textures, some of which have this subdivision of smaller textures, it is a pretty complex task to write it in such a way that it is unrollable without making two separate loops for the two types of textures. Which in turn makes maintaining the shader code quite a hassle.\n\n![viewports](\/assets\/img\/blog\/light2-viewports-grid.png){:.thumbnail}{:.with-caption}\n*Image from the internal rendering in Three.js. The grid is added to clarify what happens. The grid locations correspond with +x, &minus;x, +y, &minus;y, +z and &minus;z axis, going from top left to bottom right.*\n\nThe second approach, the cube texture, was scrapped halfway through development. While the solution seemed obvious and also is used in the industry, it was very hard to debug.\nBoth Firefox' native canvas debugger and Chrome's WebGL debugger, called the [WebGL Inspector], did not render the cube texture properly. We could observe the switching of framebuffers (which are like internal screens to draw on) but they stayed blank while the draw calls proceeded. This means Three.js did not cull them and they should have shown up on the framebuffer. With no way to debug this step and no output it would be ill-advised to continue to develop this method.\n\n![cube depth map](\/assets\/img\/blog\/light2-shadow-cube.png){:.thumbnail}{:.with-caption}\n*Image taken from [devmaster.net] explaining shadow mapping using cube maps.*\n\nThe final approach is the dual-paraboloid shadow mapping. This approach takes two textures per point light. The [previous blog post] talked about one, but this proved to be incorrect. This fact would make it less ideal than the other two approaches. On top of that, the implementation is rather complex. If we had complete control over the OpenGL code this could be a solution, but figuring out where to adapt the Three.js code and the shaders would probably turn out to be a struggle. As it would also involve a transformation to paraboloid space it would be really hard to debug. All this would be required for a lesser effect than the other - hopefully more simple - methods, like the larger texture with viewports.\n\n![paraboloid transformation](\/assets\/img\/blog\/light2-paraboloid-transformation.png){:.thumbnail}{:.with-caption}*Image taken from [gamedevelop.eu] explaining the paraboloid transformation.*\n\n### The most favorable approach\nIn conclusion the best way to make point light shadows work, without going over the texture-sampler limit or spending too much time, is the \"large texture with viewports\" approach. This means we have to duplicate some code in the shader and implement two loops to do shadow calculation: one calculating shadows for all the spot lights and one for all the point lights.\n\nAfter implementing this strategy we ran into another problem. This time the number of varying variables ([GLSL standard] page 31) in the shaders exceeded the WebGL implementation register limit. This limit in Chrome is fixed at 16. This meant we could only have one point light with shadows, which is even fewer than when we used the naive implementation. In Firefox the limit is higher which results from it being hardware implementation defined. On my - basic - hardware, it works smoothly with two point lights, but the performance starts to suffer when enabling three or more point lights. The result is shown in the video below.\n\nThe reason for this is that the hardware implementation of the fragment shaders only has a couple of \"fast registers\". These are actually separate hardware implementations of real registers which allow fast access to the data stored within. If you exceed this hardware limit, values normally stored in these fast registers will be stored in \"slow registers\". These are implemented by storing them in Video RAM, which is much slower relative to the fast registers.\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/133734871\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>{: .with-caption}\n*Shadows from point lights in our demo*\n\n### Conclusion\nCan we use these results for something practical? Yes, in Firefox this demo will run in real-time with a couple of point lights IF your hardware has some extra \"fast registers\". If you want to use more than a couple point lights, you can still use this implementation to generate screenshots of scenes that give a nice impression of the shadows being cast (in a garden, in a living room etc).\n\nFor an extensive, real-time solution you will need above average desktop hardware. Consumers using popular devices (smartphones, tablets, laptops) are obviously not part of the target audience. However, practical applications are still to found in - for example - fixed setups like a presentation in an exhibition stand.\n\n\n\n[GLSL standard]: https:\/\/www.khronos.org\/files\/opengles_shading_language.pdf#page=37\n[devmaster.net]: http:\/\/devmaster.net\/p\/3002\/shader-effects-shadow-mapping\n[gamedevelop.eu]: http:\/\/gamedevelop.eu\/en\/tutorials\/dual-paraboloid-shadow-mapping.htm\n[WebGL Inspector]: http:\/\/benvanik.github.io\/WebGL-Inspector\/ \"WebGL inspector homepage\"\n[Three.js]: http:\/\/threejs.org\/ \"three.js homepage\"\n[previous blog post]: \/2015\/05\/21\/point-light-shadows-in-threejs\/\n[a 3D project]: \/2015\/08\/14\/intelligent-3d-design\/"
        },
        {
            "layout": "post",
            "title": "\"Adding artificial intelligence to 3D design\"",
            "date": "\"2015-08-14\"",
            "thumb": "yga-beregening2-5.png",
            "leadimg": "yga-beregening2-5.png",
            "tags": "intelligent advice, knowledge based systems, 3d framework, heat map",
            "author": "Bram",
            "contact": "Bram",
            "about": "AI and our 2d framework",
            "nerd": "2",
            "content": "In 2014 we won an innovation grant from the province of Gelderland based on our proposal to provide 'intelligent' gardening advice to users of [Draw Your Garden](http:\/\/tekenjetuin.nl) (Dutch: Teken Je Tuin). We created this web application for one of our clients and we have been gradually expanding it since its release. In the app users can both design their garden and view it in 3D as well as order products and contact gardeners, who in turn can submit proposals based on the users' design.\n\nIn this article we will give you an overview of how we created methods that allow users to design their gardens intelligently. We think these methods could translate nicely to domains outside gardening. We are curious if you feel the same after reading this article.\n\n![Draw Your Garden](\/assets\/img\/blog\/tekenjetuin.png){:.with-caption}\n*Draw Your Garden \/ Teken je Tuin*\n\nPartnered with our client we proposed to develop a number of prototypes aimed at assisting users in several areas which hitherto required extensive expert knowledge. Appropriately, we named this project 'Your GardenAssistant'.\n\nFor the project we focused on three areas of garden design:\n\n* **Advice on watering a garden**; where to place different kinds of sprinklers, what kind of plants need more water, which types of sprinklers are suited to different parts of the garden and how sprinklers should be connected to what type of water source.\n* **The presence and influence of sunlight and shadow in the garden**; to show which areas of the garden receive most light in different seasons and parts of the day, where to locate a terrace and where to plant different sorts of plant.\n* **Advice on the properties of plants and trees;** which plants would prosper in different parts of the garden, how many plants flower this month, what plants are edible or poisonous.\n\n### Domain knowledge and user feedback\n\nThe first thing we concluded was that it would be crucial to involve different kinds of experts, gardeners and users in the project. We realized we needed to include them from the very start of the project, and to check back with them regularly to decide how to proceed.\n\n![The Plant Advice Prototype](\/assets\/img\/blog\/yga-aanpak.png)\n\n### 3D design\n\nThe preliminary task was to create a suitable 3D design & drawing application based on our [3D Framework](\/3d-framework\/). Our approach is to use a full 3D environment with an top-down view for drawing. This view, a kind of orthographic projection, provides the advantages of 'flat', easy-to-use interaction during drawing & design, while the full 3D experience is just one camera shift away.\n\n![Design tool](\/assets\/img\/blog\/tjt-2d.png){:.with-caption}\n*Drawing in full 3D with a top-down perspective.*\n\n![Design tool](\/assets\/img\/blog\/tjt-3d.png){:.with-caption}\n*The same garden, but rotated slightly*\n\n### Watering\n\nTo be able to give advice about watering a garden we created a 'provided graph' and a 'moisture graph', to store how much water the sprinklers provided, and how much water different parts of the garden required.\nWe used these to create a wizard designed to help the user choose both the right type of sprinker and a water source, and to connect these two, all in a few easy steps.\n\n![The Plant Advice Prototype](\/assets\/img\/blog\/water2.png){:.with-caption}\n*Provided graph: how much water is provided. Note that the sprinkler on the bottom-right is not connected to a water source.*\n\n![The Plant Advice Prototype](\/assets\/img\/blog\/water1.png){:.with-caption}\n*Moisture graph: green areas receive enough water, red areas too little or possibly too much (if they do not require water, like the terrace).*\n\n### Sunlight and shadows\n\nThe first thing here was to create a 'heat map' to represent the areas of the garden with the most sun exposure. To do this, we created a realistic simulation of the sun in the 3D representation of the garden. This process can be seen in more detail [in this simulation](\/3d-framework\/#3d-simulatie). The result of the 3D simulation, the 'heat map', is used later on when the user adds plants to the garden design: we can quickly look up the amount of sunlight at the precise spot the plant is placed.\nApart from the heat map we also created a nice user-friendly interface which allows users to instantly see which parts of the garden receive the most sun in a selected season and hour of the day.\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/136615103\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>{: .with-caption}\n*3D sunlight simulation*\n\nSide note: during the project we also looked into artificial light sources. If you are a developer you might be interested in our findings about [Point light shadows in Three.js](\/2015\/05\/21\/point-light-shadows-in-threejs\/). Three.js is the javascript 3D library most widely used to create 3D web apps with WebGL.\n\n### Plant advice\n\nGiving intelligent advice about which plants to use in your garden, and where best to place them, was arguably the most challenging part of the project. Especially since unwanted advice can very easily irritate a user a great deal.\n\nBased on feedback from our focus groups and surveys we decided to adopt an approach inspired by knowledge-based systems. We created a modular system of multiple 'Advisors', all of which provide advice or warnings based on a simple rule. For instance, we created a GrowthAdvisor based on the simple rule that fast-growing plants should not be placed too close together. This way we could easily create many more advisors based on all sorts of simple rules.\n\n![Advice](\/assets\/img\/blog\/Assistant.jpg){:.with-caption}\n*An early mockup of the advice UI*\n\nIn addition to our conceptual and technical efforts we also put a lot of thought into UI approaches, i.e. how to best present the most relevant advice. We hope to come back to this in a future blog.\n\n### Conclusions\nCreating a design application is not an easy task. Creating a design application in which users receive meaningful advice during the design process is, well, a major challenge. The grant gave us the opportunity to seriously engage these challenges.\nLooking back we are very happy with the results. The translation from data (i.e. plant properties as well as data generated from simulations) to guidance, by means of advisors, turned out to be both feasible and elegant.  \nLooking at other domains, we feel this approach is applicable to design apps and product configurators in a great variety of fields. Don't hesitate to [contact us](\/#contact) if you want to explore the possibilities. We look forward to working on a challenging project like this one in the future.\n\n### Thanks\nWe would like to thank all the participants of this project, in particular:\n\n* The province of Gelderland, for making the project possible\n* Teken je tuin, our partner for this project\n* All participants from the focus groups and test groups\n* The users of Draw Your Garden for providing us with useful feedback"
        },
        {
            "layout": "post",
            "title": "\"Some fun with physics in Three.js\"",
            "date": "\"2015-09-04\"",
            "thumb": "physics-engine.jpg",
            "leadimg": "physics-engine.jpg",
            "tags": "WebGL, physics, 3D, Three.js, ammo.js",
            "author": "Dennis",
            "contact": "RubenN",
            "about": "Three.js",
            "nerd": "3",
            "content": "We all want our 3D visualisations to be as real as possible. A basic premise seems to be that they adhere to the laws of physics. No small feat! Or is it?\n\nWe decided to give it a go during a two-day programming contest. Our team's idea was to develop a web-based game where the user cycles around and has to avoid crashing into cars. To create the game, we needed a physics engine.\n\nAs could be expected, 48 hours later we found out we had been overly ambitious. There really was no game experience to speak of. However, we did manage to create a 3D world with basic physics. Hopefully, our experiences will give you some insight into using physics in 3D.\n\n![\u0394V](\/assets\/img\/blog\/physics-dv.png){:.with-caption}*Image taken from our simplified 3d representation of the city of Nijmegen*\n\n### Making a game with Physijs\n\nOur goal was to make a small game in which you cycle through Nijmegen and have to avoid being run over by cars.\nWe first looked into some frameworks that offer physics simulation in the browser. We found [Physijs] which integrates with [Three.js] and uses [ammo.js], which is a javascript port of [bullet].\n\nPhysijs runs all the physics simulation in a web worker. The worker makes sure that it does not interfere with the renderloop. To set up Physijs you need to point it to its web worker and the ammo script that it requires.\nThe next step is to create a Physijs.Scene and call the simulate function of that scene in your update function.\n\nAfter having done the basic setup, we needed to add physics objects. To do this we needed to create Physijs meshes instead of the default Three.Meshes. There are a couple of Physijs meshes but the most useful, lightweight meshes are Physijs.BoxMesh, which uses the bounding box as physics object, and Physijs.SphereMesh, which uses the bounding sphere.\n\n![Physijs demo](\/assets\/img\/blog\/physics-physijs-demo.png){:.with-caption}*Check out [demo of Physijs] to get a feel of what is possible. For more demos check out [Physijs]*\n\n### Crashing cars\n\nFor our little game prototype we used Physijs.BoxMesh for the cars, the surfaces and the bicycle. For the buildings we used the Physisjs.ConcaveMesh, because multiple buildings needed to be grouped together in chunks and this would cause collisions otherwise. We must note Physisjs.ConcaveMesh is a very costly physics object to have in your simulation, but luckily the performance turned out to be reasonable.\n\nIn our final demo we managed to get cars moving as physics objects, following the roads that we had extracted from the government database [TOP10NL] (in Dutch). Moving the bicycle proved to be harder, because it is not a physics object. When we integrated the bike into the physics we ran into errors concerning the web worker that simulates the physics. The main difference between the bike and the cars is that the bike should have used the 'applyCentralImpulse' function which unfortunately could not be found in the simulation and the cars used 'setLinearVelocity', which did work. During the final hour of our hackathon we still weren't able to get the bike working. The result is the hilarious demo in which you can see cars mindlessly drive around, crash into each other and then bounce out of the 'world' again.\n\n<iframe src=\"https:\/\/player.vimeo.com\/video\/133123676\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen><\/iframe>\n\n### Room for improvement\n\nAs you can see in the movie the cars behave a bit strangely when tipped over. This is because the Physijs.BoxMesh allows cars to slide on their front as the physics mesh is flat at that location. Also note that not all roads match up perfectly, because of inaccuracies in the data set. Even though the demo mainly looks funny in this state, it does show correct car\/house, car\/car, and car\/surface interaction as far as physics is concerned. It just requires more tweaking to make the cars behave in a more realistic way. Given an extra day or two, we *surely* would've succeeded in making a mind-blowing game!\n\n[demo of Physijs]: http:\/\/chandlerprall.github.io\/Physijs\/examples\/vehicle.html\/\n[Physijs]: http:\/\/chandlerprall.github.io\/Physijs\/\n[Three.js]: http:\/\/threejs.org\/\n[ammo.js]: https:\/\/github.com\/kripken\/ammo.js\/\n[bullet]: http:\/\/bulletphysics.org\/wordpress\/\n[TOP10NL]: http:\/\/www.kadaster.nl\/web\/artikel\/producten\/TOP10NL.htm"
        },
        {
            "layout": "post",
            "title": "\"Three.js Collada to JSON converter\"",
            "date": "\"8-11-2015\"",
            "thumb": "teapots-json-collada.jpg",
            "leadimg": "teapots-json-collada.jpg",
            "tags": "Collada, Three.js, 3D, JSON, WebGL",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "The Collada to JSON converter",
            "description": "Optimizing 3D models by converting Collada's to JSON",
            "github": "https",
            "nerd": "3",
            "content": "The Collada format is the most commonly used format for 3D models in Three.js. However, the Collada format is an interchange format, not a delivery format. \n\n#### Interchange vs. delivery\n\nWhere a delivery format should be as small as possible and optimized for parsing by the receiving end, an interchange format doesn't have such requirements, it should just make the exchange of models between 3D authoring tools painless. Because Collada is XML it is rather verbose. And to parse a Collada, Three.js has to loop over every node of the tree and convert it to a Three.js 3D object.\n\n#### Three.js' JSON format\n\nFor improved delivery we first looked at [glTF](http:\/\/threejs.org\/docs\/#Reference\/Loaders\/glTFLoader). Unfortunately it wasn't without flaws in our implementations. Next we decided to try Three.js' own JSON format for delivery. JSON is less verbose and because it is Three.js' own format, parsing is done in a breeze. After some fruitless experiments with Maya's Three.js JSON exporter and some existing Collada to JSON converters, we tried our luck with Three.js' built in `toJSON()` method.\n\nEvery 3D object inherits the `toJSON()` method from the class Object3D, so you can convert a loaded Collada model to JSON and then save it to disk. We wanted to wrap this idea into a Nodejs app but the [ColladaLoader](https:\/\/github.com\/mrdoob\/three.js\/blob\/master\/examples\/js\/loaders\/ColladaLoader.js) for Three.js depends on the [DOMParser](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/API\/DOMParser), and there is not yet an adequate equivalent for this in Nodejs.\n\n#### Three.js JSON converter\n\nSo we made an online converter. There are 2 versions; a version that shows the model as Collada and as JSON, and a 'headless' version that just converts the Collada. The first version is suitable if you want to convert only a few models and check the models side by side for possible conversion errors, a [Collada to JSON checker](http:\/\/data.tweedegolf.nl\/collada2json\/). If you want to convert a large number of Colladas you'd better use the second version, a headless [Collada to JSON converter](http:\/\/data.tweedegolf.nl\/collada2json_headless\/).\n\n![Teapots](\/assets\/img\/blog\/teapots-json-collada.jpg){: .with-caption}\n*All great teapots are alike*\n\n#### How it works\n\nFirst the Collada gets parsed by the DOMParser to search for textures. This is necessary because Three.js' `toJSON()` method does not include textures in the resulting JSON object.\n\nWe add the images of all found textures to the `THREE.Cache` object. By doing so we suppress error messages generated by the Collada loader.\n\nThen we use the `parse()` method of the ColladaLoader to parse the Collada model into a Three.js `Group`, and because a `Group` inherits from `Object3D` we can convert it to JSON right away.\n\nThe last step is to add the texture images to the JSON file and save the result as a Blob using `URL.createObjectURL`. All done!\n\n\n#### Code and links\n - [Collada to JSON checker](http:\/\/data.tweedegolf.nl\/collada2json\/)\n - [Collada to JSON converter](http:\/\/data.tweedegolf.nl\/collada2json_headless\/)\n - [Code on Github](https:\/\/github.com\/tweedegolf\/collada2json) (the preview and headless version have their own branch)\n - [Boris Ignjatovic, our preferred 3D artist](http:\/\/www.borisignjatovic.com\/). Thanks for helping us find the best workflow!"
        },
        {
            "layout": "post",
            "title": "\"React and Three.js\"",
            "date": "\"16-02-2016\"",
            "tags": "React, Three.js, 3D, WebGL",
            "thumb": "minecraft.jpg",
            "leadimg": "minecraft.jpg",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "the examples mentioned in this post, react-three and react-three-renderer",
            "description": "Using React for creating 3D front-ends with Three.js",
            "github": "https",
            "nerd": "5",
            "content": "In the autumn of 2015, we got to know the popular javascript library [React](https:\/\/facebook.github.io\/react\/) very well, when we used it to create the fun quiz app [B-Slash](\/#portfolio-b-slash). Soon the idea arose to research the usage of React in combination with Three.js, the leading javascript library for 3D. We've been using Three.js for some years now in our projects, for example in [Tekenjetuin](\/#portfolio-tekenjetuin). We expected that using React could improve code quality in 3D projects a lot.\n\nCurrently, there are two libaries that provide React bindings for Three.js. This post will explore their differences using working examples. We hope it will help you to make up your mind which one to choose.\n\n#### React\n\nReact has become a popular choice for creating user interfaces. React keeps a virtual DOM and changes in the UI are applied to this virtual DOM first. Then React calculates the minimal set of changes that are needed to update the real DOM to match with the virtual DOM. This process is called reconciliation. Because DOM operations are expensive, the performance benefit of React is substantial.\n\nBut there is more to React than the performance impact. Especially in combination with [Flux](https:\/\/facebook.github.io\/flux\/), [JSX](https:\/\/facebook.github.io\/react\/docs\/jsx-in-depth.html) and the [debug tools](https:\/\/facebook.github.io\/react\/blog\/2014\/01\/02\/react-chrome-developer-tools.html) for the browser it is a very powerful and yet easy to use library to create complex UI's with reusable components.\n\nWhere React ultimately creates html that is rendered by the browser, there is an increasing number of libraries that provide React bindings for libraries that render to the canvas element such as [D3.js](https:\/\/github.com\/esbullington\/react-d3), [Flipboard](https:\/\/github.com\/Flipboard\/react-canvas) and [Chart.js](https:\/\/github.com\/jhudson8\/react-chartjs). There are also bindings for [SVG](https:\/\/github.com\/brentvatne\/react-native-svg) and another interesting experiment is [gl-react](https:\/\/github.com\/ProjectSeptemberInc\/gl-react).\n\n#### React and Three.js\n\nFor Three.js there are two libraries that provide React bindings:\n\n - [react-three](https:\/\/github.com\/Izzimach\/react-three)\n - [react-three-renderer](https:\/\/github.com\/toxicFork\/react-three-renderer)\n\nThree.js keeps a virtual 3D scene in memory which is rendered to the WebGL context of the canvas element every time you call the render method. The render method completely clears the canvas and creates the complete scene anew, even when nothing has changed.\n\nTherefor we have nothing to gain performance-wise when using React with Three.js, but there is still plenty reason to use it. React encourages you to create components and move state out of components as much as possible, resulting in cleaner, better to maintain code, and the JSX notation gives you a very clear overview of the hierarchical structure of the components in your 3D scene as we will see in the code examples in the next chapter.\n\n\n#### Two libraries compared\n\nReact-three is written in es5, react-three-renderer is newer and written in es6. The following code examples, that both create a simple cube, show us the differences between the libraries. First react-three:\n\n{% highlight xml %}\n\n  import React3 from 'react-three';\n\n  let Scene = React3.Scene\n  let Camera = React3.Camera;\n  let AmbientLight = React3.AmbientLight;\n  let Mesh = React3.Mesh;\n\n  <Scene\n    width={window.innerWidth}\n    height={window.innerHeight}\n  >\n    <Camera\n      aspect={window.innerWidth \/ window.innerHeight}\n      far={1000}\n      fov={50}\n      near={1}\n    \/>\n    <AmbientLight\n      color={this.props.color}\n      intensity={this.props.intensity}\n    \/>\n    <Mesh\n      position={this.props.position}\n      geometry={new THREE.BoxGeometry(this.props.size, this.props.size, this.props.size)}\n      material={new THREE.MeshBasicMaterial({color: this.props.color})}\n    \/>\n  \/>\n{% endhighlight %}\n\n\nAnd now the same in react-three-renderer:\n\n{% highlight xml %}\n\n  import Scene from 'react-three-renderer'\n\n  <Scene\n    width={window.innerWidth}\n    height={window.innerHeight}\n  >\n    <perspectiveCamera\n      aspect={window.innerWidth \/ window.innerHeight}\n      far={1000}\n      fov={50}\n      near={1}\n    \/>\n    <ambientLight\n      color={this.props.color}\n    \/>\n    <mesh\n      position={this.props.position}\n      <boxGeometry\n        width={this.props.size}\n        height={this.props.size}\n        depth={this.props.size}\n      \/>\n      <\/meshBasicMaterial\n        color={this.props.color}\n      \/>\n    \/>\n  \/>\n{% endhighlight %}\n\n\nWe see two obvious differences:\n\n**1)** In react-three we import one object and this object contains all available components. I have given the components the same name as the properties of the imported object, but I could have used any name. The naming convention in React commands us to write custom components starting with an uppercase, which I obied willingly.\n\nIn react-three-renderer we import one component and the available components are known within this component\/tag. This is because react-three-renderer uses internal components, similar to `div`, `span` and so on. Note that the names of the components start with lowercases.\n\n\n**2)** In react-three the properties geometry and material of the `Mesh` component are instances of the corresponding Three.js classes whereas in react-three-renderer both the geometry and the material are components as well.\n\nReact-three has only 17 components, but react-three-renderer strives to create components for every (relevant) Three.js class, thus gaining a higher granularity.\n\n\n#### Creating components\n\nThe following example is a Minecraft character configurator that we can use to change the sizes of all the cubes that the character consists of.\n\n![Minecraft character configurator](\/assets\/img\/blog\/minecraft_configurator_screen.png){: .with-caption}\n*Screenshot of the Minecraft character configurator*\n\nIt shows you how easy it is to create 3D components with both libraries and how your code benefits from using React both in terms of being well-organised and maintainable.\n\nAll code is available at [github](https:\/\/github.com\/tweedegolf\/minecraft-character-configurator) and you can find the live examples [here](http:\/\/data.tweedegolf.nl\/minecraft\/).\n\nThe code of the main component looks as follows:\n\n{% highlight javascript %}\n\n  <Controls\n    headSize={this.state.headSize}\n    bodyWidth={this.state.bodyWidth}\n    bodyHeight={this.state.bodyHeight}\n    bodyDepth={this.state.bodyDepth}\n    armLength={this.state.armLength}\n    armSize={this.state.armSize}\n    legLength={this.state.legLength}\n    legSize={this.state.legSize}\n  \/>\n  <Scene3D\n    sliderBusy={this.state.sliderBusy}\n    cameraPosition={this.state.cameraPosition}\n    cameraQuaternion={this.state.cameraQuaternion}\n  >\n    <World\n      position={new THREE.Vector3(0, 0, 0)}\n      worldRotation={this.state.worldRotation}\n    >\n      <Minecraft\n        key={THREE.Math.generateUUID()}\n        position={new THREE.Vector3(0, 0, 0)}\n        quaternion={new THREE.Quaternion()}\n        scale={new THREE.Vector3(1, 1, 1)}\n        config={this.state.config}\n      \/>\n    <\/World>\n  <\/Scene3D>\n\n{% endhighlight %}\n\n\nFirst we create a section that contains all controls, then we create the scenegraph containing a plane (World) on which the Minecraft character gets placed. As you can see all code specific to the Minecraft character is tucked away in its own component, leaving the hierarchal structure very clear despite its complexity.\n\nWhen we take a look at the code of the Minecraft character component we see how much complexity is actually abstracted away:\n\n{% highlight javascript %}\n\n  <group\n    key={'character'}\n    position={this.props.position}\n    quaternion={this.props.quaternion}\n    scale={this.props.scale}\n  >\n    <Box\n      key={'head'}\n      size={config.head.size}\n      color={config.head.color}\n      position={config.head.position}\n    \/>\n    <Box\n      key={'body'}\n      size={config.body.size}\n      color={config.body.color}\n      position={config.body.position}\n    \/>\n    <Box\n      key={'leftLeg'}\n      size={config.leftLeg.size}\n      color={config.leftLeg.color}\n      position={config.leftLeg.position}\n    \/>\n    <Box\n      key={'rightLeg'}\n      size={config.rightLeg.size}\n      color={config.rightLeg.color}\n      position={config.rightLeg.position}\n    \/>\n    <Box\n      key={'leftArm'}\n      size={config.leftArm.size}\n      color={config.leftArm.color}\n      position={config.leftArm.position}\n    \/>\n    <Box\n      key={'rightArm'}\n      size={config.rightArm.size}\n      color={config.rightArm.color}\n      position={config.rightArm.position}\n    \/>\n  <\/group>\n\n{% endhighlight %}\n\nHere we see a component named Box which is some wrapper code around a cube. By using this component we not only reduce the amount of code in the Minecraft character module, we also abstract away differences between the 2 libraries.\n\nThis means that we can use the Minecraft character component both in projects that use react-three and in projects that use react-three-renderer.\n\nTo see the different implementations of the Box component please take a look at the code on github: [react-three](https:\/\/github.com\/tweedegolf\/minecraft-character-configurator\/blob\/master\/react-three\/js\/components\/three\/box.react.js) and [react-three-renderer](https:\/\/github.com\/tweedegolf\/minecraft-character-configurator\/blob\/master\/react-three-renderer\/js\/components\/three\/box.react.js).\n\n\n#### Importing models\n\nThe model loaders for Three.js load the various 3D formats (Collada, FBX, Obj, JSON, and so on) and parse them into Three.js objects that can be added to the scene right away. This is very convenient when you use Three.js without React bindings, but it requires an extra conversion step when we do use React bindings because we need to parse the Three.js object into components.\n\nI have written some utility code for this which is available at [github](https:\/\/github.com\/tweedegolf\/parsed-model). You can find two working examples of how to use this code with both libraries in a separate repository at [github](https:\/\/github.com\/tweedegolf\/parsed_model_examples).\n\nThe utility is a parser and a loader in one and this is how you use it:\n\n{% highlight javascript %}\n\n  let parsedModel = new ParsedModel();\n  parsedModel.load('path\/to\/model.json');\n\n{% endhighlight %}\n\nAfter the model is loaded it is parsed right-away. During the parsing step a map containing all geometries is generated. All these geometries are merged into one single large geometry as well and for this merged geometry a multi-material is created.\n\nNow we can use it in a React component, in react-three like so:\n\n{% highlight javascript %}\n\n  <Mesh\n    geometry={parsedModel.mergedGeometry}\n    material={parsedModel.multiMaterial}\n  \/>\n\n{% endhighlight %}\n\n\nIn react-three-renderer we need more code, on the one hand because multi-materials are not (yet) supported so we can not use the merged geometry, and on the other hand because of its higher granularity:\n\n\n{% highlight javascript %}\n\n  let meshes = [];\n\n  parsedModel.geometries.forEach((geometry, uuid) => {\n    \/\/ get the right material for this geometry using the material index\n    let material = parsedModel.materialArray[materialIndices.get(uuid)];\n\n    meshes.push(\n      <mesh\n        key={uuid}\n      >\n        <geometry\n          vertices={geometry.vertices}\n          faces={geometry.faces}\n        \/>\n        {createMaterial(material)}\n      <\/mesh>\n    );\n  })\n\n  <group>\n    {meshes}\n  <\/group>\n\n{% endhighlight %}\n\nThe `createMaterial` method parses a Three.js material into a react-three-renderer component, see this code at [github](https:\/\/github.com\/tweedegolf\/parsed-model\/blob\/master\/create_material.js).\n\n\n#### Pros and cons\n\nUsing React-bindings for Three.js results in very clean code. Usually you don't have a hierarchical overview of your 3D scene, but with React your scene is clearly laid out in a tree of components. As as bonus, you can debug your scene with the React browser tools.\n\nAs we have seen in the Minecraft character configurator, using React is very efficient for applications that use composite components, and we have seen how smoothly React GUI controls can be connected to a 3D scene.\n\nIn applications with a flat structure, for instance when you have a lot of 3D objects placed on the scene, the JSX code of your scenegraph becomes merely a long list which might be as hard to understand as the original Three.js representation of the scenegraph.\n\nHowever, with React you can split up such a long list in a breeze, for example by categorizing the 3D objects:\n\n{% highlight javascript %}\n\n<Scene3D>\n  <models type={\"characters\"} \/>\n  <models type={\"buildings\"} \/>\n  <models type={\"trees\"} \/>\n<\/Scene3D>\n\n{% endhighlight %}\n\n\nSometimes using React requires some extra steps, for instance when loading 3D models, and sometimes it might take a bit time to find the right way of implementing common Three.js functionality like for instance user controls or calling Three.js' own render method manually.\n\nTo elaborate on the latter example: by default both react-three and react-three-renderer call Three.js' render function continuously by passing it to `Window.requestAnimationFrame()`. While this is a good choice for 3D games and animations, it is might be overkill in applications that have a more static scene like applications that simply show 3D models, or our Minecraft character configurator. In both libraries it is possible to turn off automatic rendering by setting a parameter on the scenegraph component, as you can see in the code of the Minecraft character configurator.\n\n\n#### Conclusion\n\nFor the types of project that I have discussed above I would definitely recommend using React bindings for Three.js. Not only your code will be better set up and thus better maintainable, it will also speed up your work significantly once you have acquainted yourself with the workflow of React as well.\n\nWhether you should use react-three or react-three-renderer depends on your project. Both libraries are relatively new but as you can see on Github the code gets updated on a weekly basis, and moreover there are lively discussions going on in the issue trackers and issues and suggestions are quite swiftly picked up.\n\nSome final remarks that can help you make up your mind:\n\n- react-three depends on Three.js r72 React version 0.14.2, react-three-renderer works with the most recent versions of both Three.js and React.\n- react-three-renderer has not yet implemented all Three.js features, react-three does (mainly because its lesser granularity).\n- in react-three the ray caster doesn't work i.c.w. controls like the OrbitControls, in react-three-renderer it does.\n- both libraries provide excellent examples, studying these will give you a good grasp of the basic principles.\n\nDon't hesitate to get in touch with us, if you have any questions or remarks about this post. Feedback is much appreciated."
        },
        {
            "layout": "post",
            "title": "\"Augmented Reality with web technologies\"",
            "date": "\"24-05-2016\"",
            "tags": "Three.js, 3D, WebGL, Augmented Reality, JSAruco, JSARToolkit, Augmented Web",
            "thumb": "jsaruco.jpg",
            "leadimg": "jsaruco.jpg",
            "author": "Daniel",
            "contact": "Daniel",
            "about": "augmented reality with web technologies",
            "description": "Using solely web technologies for creating augmented reality applications",
            "github": "https",
            "nerd": "3",
            "content": "#### Augmented Reality with web technologies\n\nWith the start of the implementation of the WebRTC API around 2012, javascript developers gained access to the video and audio streams coming from webcams and microphones. This paved the way for augmented reality (AR) applications that were build solely with web technologies. Using webtechnologies for AR is also called \"the augmented web\".\n\nMost AR applications use a webcam feed that gets analyzed for certain patterns, for instance a simple color field, a movement or a marker. Nowadays there are several other ways to augment the reality of a webpage, for instance using geolocation or devices such as the Leapmotion.\n\nIn this post I will focus on AR with markers.\n\n#### Two libraries\n\nWhile some developers have created their own AR libraries, most developers use either [JSAruco](https:\/\/github.com\/jcmellado\/js-aruco) or [JSARToolkit](https:\/\/github.com\/kig\/JSARToolKit). Both libraries are javascript ports from ancient C++ libraries. JSAruco is based on [OpenCV](http:\/\/opencv.org\/) and JSARToolkit is a port of [ARToolkit](http:\/\/www.hitl.washington.edu\/artoolkit\/) via the in-between ports NyARToolkit (Java) and FLARToolkit (Actionscript).\n\nThe inner-workings of the libraries is as follows: a snapshot of the video feed is taken by copying image data from the video to a canvas on every animation frame. This image data gets analyzed for markers, and the position and rotation of every detected marker is returned. This information can subsequently be used to render a 3D object on top of the video feed, thus augmenting the reality.\n\nThree.js works very well with both JSAruco and JSARToolkit and I made 2 simple examples that show you how to use the libraries with Three.js, the code and some markers are available at [Github](https:\/\/github.com\/tweedegolf\/web-ar).\n\n![AR](\/assets\/img\/blog\/jsaruco.jpg){: .with-caption}\n*3D model rendered on a marker*\n\n\n#### Markers\n\nA marker is usually a grid of small black and white squares and there are certain rules for how these black and white squares must be patched together. Diogok has put [code on github](https:\/\/github.com\/diogok\/js-aruco-markers) that generates all possible JSAruco markers.\n\nNote that JSAruco markers can not be used with JSARToolkit; you have to use the markers that you can find in the repository on Github, in the [markers folder](https:\/\/github.com\/kig\/JSARToolKit\/tree\/master\/demos\/markers).\n\nBoth libraries support multiple markers, which means that each distinct marker gets its own id, and this id can be used to couple a model (or an action) to a specific marker.\n\nFor instance I made this small test using multiple markers:\n\n<video height=\"360\" controls>\n  <source src=\"http:\/\/data.tweedegolf.nl\/web-ar\/tjt-web-ar.mp4\">\n<\/video>\n\n\n#### Conditions\n\nIn the process of analyzing, the image is turned into an inverted plain black and white image. This means that a pixel is either white or black and this makes it very easy to detect a marker. For the best results, good bright lighting is mandatory. Also putting the marker on a surface with a plain color is recommended. If possible, using backlight is ideal. In general you should turn off the auto focus of your webcam.\n\n![Marker detection](\/assets\/img\/blog\/inverted.jpg){: .with-caption}\n*Marker detection*\n\n\n#### Performance\n\nIn JSARToolkit you can set the size of the image data that will be processed: parsing smaller images is faster but on the other hand smaller images have less detail. Besides tweaking the size of the image being processed, you can set the threshold, which is the color value that classifies whether a pixel will become white or black.\n\nIn JSAruco the size of the image data has to match the size of the canvas that you use to render the 3D scene (in our case: where we render the Three.js scene). I have noticed that if the width of the canvas is more than about 700 pixels, JSAruco starts to have difficulties detecting markers, and the wider the canvas, the more severe this problem becomes.\n\nIn general JSARToolkit performs better than JSAruco, but both libraries suffer from missed or wrongly positioned markers, resulting in an unsteady presentation. You can compare both libraries yourself using the simple test applications that I mentioned earlier. Code is at [Github](https:\/\/github.com\/tweedegolf\/web-ar).\n\n\n#### Web or native\n\nOn iOS you don't have access to the streams coming from a camera or a microphone due to restrictions put in place by Apple. So on this platform it is impossible to create an AR application with only web technologies. Since mobile devices have become ubiquitous, you see an increasing number of native AR libraries for mobile platforms appear on Github, especially for iOS.\n\nThe benefits of native are twofold: better performance and control over all camera settings (contrast, brightness, auto focus, etc.). Better performance means faster and more accurate marker detection and control over camera settings provide tools for optimizing the incoming feed.\n\nMoreover you can use the light sensor of your device to detect the light intensity and adjust the camera settings accordingly. Currently you can't use the light sensor API on iOS, but on Android, Ubuntu, FirefoxOS and Windows you can.\n\n\n#### Conclusion\n\nTechnically you can build an AR application using web technologies but the performance isn't as good as native AR apps such as Layar and Roomle. For some applications web technologies might suffice, for instance art installations or applications that just want to show the possibilities of AR.\n\nThe advantage of using web technologies is obvious: it is much simpler to set up an application and it runs on any platform (iOS being the sad exception).\n\nThe lesser performance is partly because analyzing the image data is done in the main javascript thread, and partly because the lack of control over the camera settings which leads to a poor quality of the incoming feed, for instance due to bad or fluctuating light conditions.\n\nOn the short term using webworkers may improve the analyzing and detection step, and on the longer term the ever improving performance of browsers will eventually lead to a more reliable marker detection.\n\nFurthermore Web API's keep evolving so in the near future we might get more control over the camera settings via javascript. The draft version of the [MediaCapture API](http:\/\/w3c.github.io\/mediacapture-main\/#dictionary-mediatrackcapabilities-members) already shows some useful future capabilities. Also there is a draft Web API for the [light sensor](https:\/\/w3c.github.io\/ambient-light\/) that is currently only implemented in [Firefox](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/API\/DeviceLightEvent\/Using_light_sensors).\n\nThe future of the augmented web looks bright."
        }
    ]
}